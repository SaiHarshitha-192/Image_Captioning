# -*- coding: utf-8 -*-
"""Word2Vec2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pp97xDfkkOAVVhgrnXe_ipuHg24Brsip
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install nltk
!pip install -q keras-ocr

import nltk
nltk.download('punkt')

import os
import keras_ocr
import matplotlib.pyplot as plt
from gensim.models import KeyedVectors
import os
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize

folder_path = '/content/drive/MyDrive/major_project/30k/Divided_Images/part1_text'
WORKING_DIR = '/content/drive/MyDrive/major_project/Working/30k'

# Load text from each file in the folder
words = []
for filename in os.listdir(folder_path):
    if filename.endswith('.txt'):
        with open(os.path.join(folder_path, filename), 'r') as file:
            text = file.read()
            words.append(text)
print(words)
print(len(words))

# Load text from each file in the folder
data = {}
for filename in os.listdir(folder_path):
    if filename.endswith('.txt'):
        with open(os.path.join(folder_path, filename), 'r') as file:
            text = file.read()
            data[filename] = text
print(data)
print(len(data))

from gensim.models import Word2Vec
import numpy as np

# Preprocess the text data
sentences = []
for file_name, text in data.items():
    words = text.split('\n')
    words = [word for word in words if word]  # Remove empty strings
    sentences.append(words)

print(sentences)

# Train Word2Vec model
model = Word2Vec(sentences, vector_size=300, window=5, min_count=0, workers=4)

import re

# Preprocess the text data and extract individual words
words = []
for file_name, text in data.items():
    file_words = text.split('\n')
    file_words = [re.sub(r'\W+', '', word) for word in file_words]  # Remove non-word characters
    file_words = [word for word in file_words if word]  # Remove empty strings
    words.extend(file_words)

# Train Word2Vec model
model = Word2Vec([words], vector_size=300, window=5, min_count=1, workers=4)

# Save the trained model
model.save("word2vec_model")

# Load the trained model
# model = Word2Vec.load("word2vec_model")

# Obtain embeddings for the words in the dictionary
word_embeddings = {}
for file_name, text in data.items():
    file_words = text.split('\n')
    file_words = [re.sub(r'\W+', '', word) for word in file_words]  # Remove non-word characters
    file_words = [word for word in file_words if word]  # Remove empty strings

    file_embeddings = []
    for word in file_words:
        try:
            embedding = model.wv[word]
        except KeyError:
            embedding = np.zeros(300)
        file_embeddings.append(embedding)

    word_embeddings[file_name] = file_embeddings

# Print word embeddings for each file
for file_name, embeddings in word_embeddings.items():
    print(file_name, embeddings)

print(len(word_embeddings))

import numpy as np

# Create a new dictionary to store reshaped embeddings
reshaped_embeddings = {}

# Iterate over items in the dictionary
for key, value in word_embeddings.items():
    if not value:  # If the value is an empty list
        reshaped_embeddings[key] = np.zeros((0, 300))  # Create a (0, 300) array
    else:
        # Convert the list of embeddings to a NumPy array
        embeddings_array = np.array(value)

        # Reshape the array to the desired shape
        reshaped_shape = (len(value), len(value[0]))
        reshaped_array = embeddings_array.reshape(reshaped_shape)

        # Store the reshaped array in the new dictionary
        reshaped_embeddings[key] = reshaped_array

# Print the reshaped embeddings
for key, value in reshaped_embeddings.items():
    print(key, value.shape)

import pickle
pickle_filename = WORKING_DIR+'/text_features.pkl'

# Load existing data from the pickle file
with open(pickle_filename, 'rb') as f:
    existing_data = pickle.load(f)

# Assume new_data is the additional data you want to add
new_data = reshaped_embeddings # Change this to the new data you want to add

# Update existing_data with new_data
existing_data.update(new_data)

# Dump the combined data back into the pickle file
with open(pickle_filename, 'wb') as f:
    pickle.dump(existing_data, f)

'''import pickle

pickle_filename = WORKING_DIR+'/text_features.pkl'

# print("Pickle file updated successfully:", pickle_filename)

# Dump the additional dictionary into the pickle file
with open(os.path.join(WORKING_DIR, 'text_features.pkl'), "wb") as f:  # Use "ab" (append binary) mode
    pickle.dump(reshaped_embeddings, f)

print("Additional values added to pickle file:", pickle_filename)'''

# load features from pickle
with open(os.path.join(WORKING_DIR, 'text_features.pkl'), 'rb') as f:
    features = pickle.load(f)
    print(len(features))
    print(features.keys())
    a=features['967719295_3257695095.txt']
    print(a.shape)
    print(a)
    print('======================================================================================================')
    a=features['2021613437_d99731f986.txt']
    print(a.shape)
    print(a)
    print('======================================================================================================')
    a=features['3336808362_c17837afd8.txt']
    print(a.shape)
    print(a)
    a=features['2831672255_d779807c14.txt']
    print(a.shape)
    print(a)